{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "349d4a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install ultralytics --quiet\n",
    "!pip install opencv-python --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751ebf2",
   "metadata": {},
   "source": [
    "# YOLO Pose Tracking - Code Documentation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This Python script implements real-time human pose estimation and tracking using YOLOv11 pose detection model. The system processes video files to detect and track human poses, generating annotated output videos with skeletal keypoints overlaid on detected persons.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Human Pose Detection: Uses YOLOv11 pose model for accurate keypoint detection\n",
    "- Multi-Person Tracking: Tracks multiple persons across video frames\n",
    "- Performance Optimization: Includes frame skipping and downscaling for faster processing\n",
    "- Configurable Parameters: Easily adjustable settings for different use cases\n",
    "- Video Output: Saves annotated videos with pose overlays\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import time\n",
    "```\n",
    "\n",
    "### Required Libraries\n",
    "\n",
    "- ultralytics: YOLOv11 implementation and model loading\n",
    "- opencv-python (cv2): Video processing and computer vision operations\n",
    "- time: Performance timing and benchmarking\n",
    "\n",
    "## Configuration Parameters\n",
    "\n",
    "- MODEL_PATH: 'yolo11x-pose.pt' - Path to the YOLOv11 pose model file\n",
    "- VIDEO_PATH: \"/home/sourav/code/.../sample-1.mp4\" - Input video file path\n",
    "- OUTPUT_PATH: \"/home/sourav/code/.../sample-1_output.mp4\" - Output video file path\n",
    "- DOWNSCALE: 0.5 - Frame resize factor (0.5 = 50% of original size)\n",
    "- FRAME_SKIP: 1 - Process every nth frame (1 = process all frames)\n",
    "- DISPLAY_GUI: False - Enable/disable real-time display window\n",
    "- CONF_THRESHOLD: 0.6 - Confidence threshold for pose detection\n",
    "\n",
    "## Code Structure\n",
    "\n",
    "### Step 1: Library Imports\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import time\n",
    "```\n",
    "\n",
    "### Step 2: Configuration Setup\n",
    "\n",
    "Defines all configurable parameters including model path, video paths, and optimization settings.\n",
    "\n",
    "### Step 3: Model Loading\n",
    "\n",
    "```python\n",
    "model = YOLO(MODEL_PATH)\n",
    "model.fuse()  # Optimize model for inference\n",
    "```\n",
    "\n",
    "- Loads the YOLOv11 pose model\n",
    "- Applies model fusion for improved inference speed\n",
    "\n",
    "### Step 4: Video Setup\n",
    "\n",
    "```python\n",
    "cap = cv2.VideoCapture(VIDEO_PATH, cv2.CAP_FFMPEG)\n",
    "```\n",
    "\n",
    "- Initializes video capture with FFMPEG backend\n",
    "- Extracts video properties (width, height, FPS)\n",
    "- Sets up video writer for output\n",
    "\n",
    "### Step 5: Processing Loop\n",
    "\n",
    "The main processing loop handles:\n",
    "\n",
    "1. Frame Reading: Captures frames from input video\n",
    "2. Frame Skipping: Processes every nth frame based on FRAME_SKIP\n",
    "3. Downscaling: Resizes frames for faster inference\n",
    "4. Pose Detection: Runs YOLO inference with tracking\n",
    "5. Annotation: Draws pose keypoints and connections\n",
    "6. Output Writing: Saves annotated frames to output video\n",
    "\n",
    "### Step 6: Resource Cleanup\n",
    "\n",
    "Properly releases video capture, writer, and destroys display windows.\n",
    "\n",
    "## Performance Optimizations\n",
    "\n",
    "### 1. Model Optimization\n",
    "\n",
    "- Uses model.fuse() to optimize the model for inference\n",
    "- Lighter YOLOv11x-pose model for balance between accuracy and speed\n",
    "\n",
    "### 2. Frame Processing\n",
    "\n",
    "- Downscaling: Reduces frame size by 50% (DOWNSCALE = 0.5)\n",
    "- Frame Skipping: Processes every frame (FRAME_SKIP = 1)\n",
    "- Confidence Threshold: Higher threshold (0.6) reduces false positives\n",
    "\n",
    "### 3. Resource Management\n",
    "\n",
    "- Disables GUI display (DISPLAY_GUI = False) for headless processing\n",
    "- Uses FFMPEG backend for efficient video reading\n",
    "\n",
    "## Model Information\n",
    "\n",
    "The script uses YOLOv11x-pose model which provides:\n",
    "\n",
    "- 17 Keypoints: Standard COCO pose keypoints\n",
    "- Multi-Person Detection: Simultaneous tracking of multiple individuals\n",
    "- High Accuracy: Balanced performance for most use cases\n",
    "- Real-time Capability: Optimized for video processing applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e23aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading model...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-pose.pt to 'yolo11x-pose.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113M/113M [00:43<00:00, 2.73MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11x-pose summary (fused): 483 layers, 58,751,308 parameters, 0 gradients, 202.8 GFLOPs\n",
      "âœ… Model loaded.\n",
      "ðŸš€ Starting optimized pose tracking...\n",
      "ðŸ–¼ï¸ Processed 100 frames. â±ï¸ Last frame time: 1.190s\n",
      "ðŸ–¼ï¸ Processed 200 frames. â±ï¸ Last frame time: 1.168s\n",
      "\n",
      "âœ… Completed. Total frames processed: 225/225\n",
      "ðŸ“ Output saved to: /home/sourav/code/my-sdk/Boilerplates/CV/VideoAnalysis/sample_videos/sample-1_output.mp4\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Import necessary libraries\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# STEP 2: Define configuration\n",
    "MODEL_PATH = 'yolo11x-pose.pt'  # Lighter model for faster inference\n",
    "VIDEO_PATH = \"/home/sourav/code/my-sdk/Boilerplates/CV/VideoAnalysis/sample_videos/sample-1.mp4\"\n",
    "OUTPUT_PATH = \"/home/sourav/code/my-sdk/Boilerplates/CV/VideoAnalysis/sample_videos/sample-1_output.mp4\"\n",
    "\n",
    "DOWNSCALE = 0.5              # Resize frame to 50%\n",
    "FRAME_SKIP = 1               # Process every nth frame\n",
    "DISPLAY_GUI = False          # Disable GUI for speed\n",
    "CONF_THRESHOLD = 0.6         # âœ…ewer detections, faster inference\n",
    "\n",
    "\n",
    "# STEP 4: Load YOLOv8 pose model\n",
    "print(\"Loading model...\")\n",
    "model = YOLO(MODEL_PATH)\n",
    "model.fuse()  \n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# STEP 5: Setup video reader and writer\n",
    "cap = cv2.VideoCapture(VIDEO_PATH, cv2.CAP_FFMPEG)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(f\"Cannot open video file at {VIDEO_PATH}\")\n",
    "\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_size = (width, height)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, frame_size)\n",
    "\n",
    "# STEP 6: Run pose estimation and tracking\n",
    "frame_count = 0\n",
    "processed_count = 0\n",
    "print(\"Starting optimized pose tracking...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames to speed up processing\n",
    "    if frame_count % FRAME_SKIP != 0:\n",
    "        continue\n",
    "\n",
    "    # Downscale frame\n",
    "    resized_frame = cv2.resize(frame, (0, 0), fx=DOWNSCALE, fy=DOWNSCALE)\n",
    "\n",
    "    # Inference with tracking\n",
    "    start_time = time.time()\n",
    "    results = model.track(resized_frame, persist=True, verbose=False, conf=CONF_THRESHOLD)\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    # Annotate and resize back to original resolution\n",
    "    annotated_frame = results[0].plot()\n",
    "    annotated_frame = cv2.resize(annotated_frame, frame_size)\n",
    "\n",
    "    # Display or write frame\n",
    "    if DISPLAY_GUI:\n",
    "        cv2.imshow(\"Pose Tracking\", annotated_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Interrupted by user.\")\n",
    "            break\n",
    "\n",
    "    out.write(annotated_frame)\n",
    "    processed_count += 1\n",
    "\n",
    "    if processed_count % 100 == 0:\n",
    "        print(f\"Processed {processed_count} frames. â±Last frame time: {inference_time:.3f}s\")\n",
    "\n",
    "# STEP 7: Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"\\nCompleted. Total frames processed: {processed_count}/{frame_count}\")\n",
    "print(f\"Output saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3cc020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
